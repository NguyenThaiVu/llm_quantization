{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70758330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gemm_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2675049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_row_int8_symmetric(mat: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric int8 quantization per row.\n",
    "    mat: (N, M) float tensor\n",
    "    Returns:\n",
    "      q_mat: (N, M) int8\n",
    "      scales: (N,) float32\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "\n",
    "    # Avoid division by zero\n",
    "    max_vals = mat.abs().amax(dim=1, keepdim=True)  # (N, 1)\n",
    "    max_vals = max_vals.clamp(min=1e-8)\n",
    "\n",
    "    scales = (max_vals / qmax).squeeze(1)          # (N,)\n",
    "    q_mat = torch.clamp(torch.round(mat / scales.unsqueeze(1)), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    return q_mat, scales.to(torch.float32)\n",
    "\n",
    "\n",
    "def quantize_col_int8_symmetric(mat: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric int8 quantization per column.\n",
    "    mat: (N, M) float tensor\n",
    "    Returns:\n",
    "      q_mat: (N, M) int8\n",
    "      scales: (M,) float32\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "\n",
    "    max_vals = mat.abs().amax(dim=0, keepdim=True)  # (1, M)\n",
    "    max_vals = max_vals.clamp(min=1e-8)\n",
    "\n",
    "    scales = (max_vals / qmax).squeeze(0)           # (M,)\n",
    "    q_mat = torch.clamp(torch.round(mat / scales.unsqueeze(0)), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    return q_mat, scales.to(torch.float32)\n",
    "    \n",
    "\n",
    "def dequant_col_int8(mat_int8: torch.Tensor,\n",
    "                      scales: torch.Tensor,\n",
    "                      out_dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    Dequantize int8 matrix per column.\n",
    "    mat_int8: (N, M) int8\n",
    "    scales: (M,) float32\n",
    "    \"\"\" \n",
    "    mat_float = mat_int8.to(torch.float32)\n",
    "    mat_dequant = mat_float * scales.unsqueeze(0)  # (N, M)\n",
    "    return mat_dequant.to(out_dtype)\n",
    "\n",
    "def dequant_int8_gemm(out_int: torch.Tensor,\n",
    "                      x_scale: torch.Tensor,\n",
    "                      w_scale: torch.Tensor,\n",
    "                      out_dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    Dequantize result of INT8 matmul:\n",
    "      out_int: (B, out_features) int32 or float32\n",
    "      x_scale: (B,) from input rows\n",
    "      w_scale: (out_features,) from weight rows/cols\n",
    "    \"\"\"\n",
    "    if out_int.dtype == torch.int32:\n",
    "        out_float = out_int.to(torch.float32)\n",
    "    else:\n",
    "        out_float = out_int\n",
    "\n",
    "    # scale = x_scale.unsqueeze(1) * w_scale.unsqueeze(0)  # (B, out_features)\n",
    "    # out = out_float * scale\n",
    "    \n",
    "    out = out_float * x_scale[:, None] * w_scale[None, :]\n",
    "    \n",
    "    return out.to(out_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7985b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, default_dtype=torch.float16):\n",
    "        super().__init__()\n",
    "        # Keep FP16 weights initially as (in_features, out_features)\n",
    "        self.w = nn.Parameter(torch.randn(in_features, out_features, dtype=default_dtype))\n",
    "\n",
    "        # For quantized weights:\n",
    "        # we'll store w_q as (out_features, in_features) to match gemm_int8's (N, K)\n",
    "        self.register_buffer(\"w_q\", torch.empty(0, dtype=torch.int8), persistent=False)\n",
    "        self.register_buffer(\"w_scale\", torch.empty(0, dtype=torch.float32), persistent=False)\n",
    "\n",
    "        self.is_quantized = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perform_weight_quantization(self):\n",
    "        # Re-orient weights: (in_features, out_features) -> (out_features, in_features)\n",
    "        w_t = self.w.t().contiguous()  # (out_features, in_features)\n",
    "\n",
    "        w_q, w_scale = quantize_row_int8_symmetric(w_t)  # row-wise over out_features\n",
    "\n",
    "        self.w_q = w_q\n",
    "        self.w_scale = w_scale\n",
    "        self.is_quantized = True\n",
    "\n",
    "        # Free FP16 weights\n",
    "        del self.w\n",
    "        self.w = None\n",
    "        print(\"CustomLinear: quantized weights to int8 and deleted float weights.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.is_quantized:\n",
    "            return x @ self.w\n",
    "\n",
    "        # Quantize activations per row: (batch, in_features)\n",
    "        x_q, x_scale = quantize_row_int8_symmetric(x)\n",
    "        out_int = gemm_int8.matmul(x_q, self.w_q, alpha=1.0)\n",
    "\n",
    "        out = dequant_int8_gemm(out_int, x_scale, self.w_scale, out_dtype=torch.float16)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc40077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, default_dtype=torch.float16):\n",
    "        super().__init__()\n",
    "        self.fc1 = CustomLinear(input_size, hidden_size, default_dtype)\n",
    "        self.fc2 = CustomLinear(hidden_size, hidden_size, default_dtype)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size, dtype=default_dtype)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def start_quantization(self):\n",
    "        # Quantize all CustomLinear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, CustomLinear):\n",
    "                m.perform_weight_quantization()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f771d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for forward pass over 100 iterations: 1.33 ms\n",
      "torch.Size([512, 8192])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 1024 * 8\n",
    "batch_size = 512\n",
    "ToyModel = CustomModel(hidden_size, hidden_size, hidden_size, default_dtype=torch.float16).cuda()\n",
    "\n",
    "input_data = torch.randn(batch_size, hidden_size, device='cuda', dtype=torch.float16)\n",
    "\n",
    "# Measure time \n",
    "torch._dynamo.reset()\n",
    "# Warm up\n",
    "for _ in range(10):\n",
    "    output = ToyModel(input_data)\n",
    "\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)    \n",
    "n_iter = 100\n",
    "start_event.record()\n",
    "for _ in range(n_iter):\n",
    "    output = ToyModel(input_data)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "avg_time = elapsed_time_ms / n_iter\n",
    "print(f\"Average time for forward pass over {n_iter} iterations: {avg_time:.2f} ms\")\n",
    "    \n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f929a274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total GFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::matmul         1.15%      26.438us        60.76%       1.402ms     700.886us       0.000us         0.00%     777.668us     388.834us           0 b           0 b      16.00 Mb           0 b             2            --  \n",
      "                                               aten::mm        57.37%       1.323ms        59.62%       1.375ms     687.667us     777.668us        64.66%     777.668us     388.834us           0 b           0 b      16.00 Mb      16.00 Mb             2       137.439  \n",
      "ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us     776.868us        64.60%     776.868us     388.434us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                           aten::linear         0.39%       9.083us         2.88%      66.333us      66.333us       0.000us         0.00%     411.394us     411.394us           0 b           0 b       8.00 Mb           0 b             1            --  \n",
      "                                            aten::addmm         1.17%      27.003us         1.37%      31.563us      31.563us     411.394us        34.21%     411.394us     411.394us           0 b           0 b       8.00 Mb       8.00 Mb             1        68.719  \n",
      "ampere_fp16_s16816gemm_fp16_128x128_ldg8_relu_f2f_st...         0.00%       0.000us         0.00%       0.000us       0.000us     411.394us        34.21%     411.394us     411.394us           0 b           0 b           0 b           0 b             1            --  \n",
      "                                             aten::relu         1.51%      34.849us         3.49%      80.405us      40.202us       0.000us         0.00%      13.568us       6.784us           0 b           0 b      16.00 Mb           0 b             2            --  \n",
      "                                        aten::clamp_min         1.42%      32.670us         1.97%      45.556us      22.778us      13.568us         1.13%      13.568us       6.784us           0 b           0 b      16.00 Mb      16.00 Mb             2            --  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      13.568us         1.13%      13.568us       6.784us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       0.800us         0.07%       0.800us       0.400us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                        cudaMemsetAsync         0.82%      18.993us         0.82%      18.993us       9.496us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.37%       8.458us         0.37%       8.458us       2.819us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             3            --  \n",
      "                                       cudaLaunchKernel         1.82%      41.879us         1.82%      41.879us       8.376us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5            --  \n",
      "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     -40.00 Mb     -40.00 Mb             5            --  \n",
      "                                                aten::t         0.46%      10.534us         1.11%      25.687us      25.687us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "                                        aten::transpose         0.50%      11.522us         0.66%      15.153us      15.153us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "                                       aten::as_strided         0.16%       3.631us         0.16%       3.631us       3.631us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "                                  cudaDeviceSynchronize        32.87%     758.367us        32.87%     758.367us     758.367us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.307ms\n",
      "Self CUDA time total: 1.203ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch profiler\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    "    profile_memory=True,\n",
    "    with_flops=True,\n",
    ") as prof:\n",
    "    output = ToyModel(input_data)\n",
    "    \n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfdc53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b22f7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomLinear: quantized weights to int8 and deleted float weights.\n",
      "CustomLinear: quantized weights to int8 and deleted float weights.\n"
     ]
    }
   ],
   "source": [
    "ToyModel.start_quantization()\n",
    "\n",
    "ToyModel = torch.compile(ToyModel, dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c3b0b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for quantized forward pass over 100 iterations: 1.00 ms\n",
      "torch.Size([512, 8192])\n"
     ]
    }
   ],
   "source": [
    "# Measure time \n",
    "torch._dynamo.reset()\n",
    "# Warm up\n",
    "for _ in range(10):\n",
    "    output = ToyModel(input_data)\n",
    "    \n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)    \n",
    "n_iter = 100\n",
    "start_event.record()\n",
    "for _ in range(n_iter):\n",
    "    output = ToyModel(input_data)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "avg_time = elapsed_time_ms / n_iter\n",
    "print(f\"Average time for quantized forward pass over {n_iter} iterations: {avg_time:.2f} ms\")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a582376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total GFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  Torch-Compiled Region         4.85%     142.288us        87.37%       2.566ms       2.566ms       0.000us         0.00%       1.030ms       1.030ms           0 b           0 b      16.00 Mb           0 b             1            --  \n",
      "                                       CompiledFunction        70.65%       2.075ms        82.52%       2.423ms       2.423ms       0.000us         0.00%       1.030ms       1.030ms           0 b           0 b      16.00 Mb           0 b             1            --  \n",
      "                            gemm_int8_CUDA::int8_matmul         1.68%      49.212us         3.52%     103.353us      51.676us     548.991us        53.32%     548.991us     274.496us           0 b           0 b      16.00 Mb           0 b             2            --  \n",
      "_ZN7cutlass6KernelINS_4gemm6kernel4GemmINS1_11thread...         0.00%       0.000us         0.00%       0.000us       0.000us     548.991us        53.32%     548.991us     274.496us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                            aten::addmm         1.79%      52.650us         2.14%      62.777us      62.777us     441.599us        42.89%     441.599us     441.599us           0 b           0 b           0 b           0 b             1        68.719  \n",
      "ampere_fp16_s16816gemm_fp16_128x128_ldg8_relu_f2f_st...         0.00%       0.000us         0.00%       0.000us       0.000us     441.599us        42.89%     441.599us     441.599us           0 b           0 b           0 b           0 b             1            --  \n",
      "                                                triton_         0.00%       0.000us         0.00%       0.000us       0.000us      39.072us         3.79%      39.072us      13.024us           0 b           0 b           0 b           0 b             3            --  \n",
      "triton_red_fused__to_copy_abs_amax_clamp_div_round_0...         3.07%      90.176us         4.29%     126.059us     126.059us      18.528us         1.80%      18.528us      18.528us           0 b           0 b           0 b           0 b             1            --  \n",
      "triton_red_fused__to_copy_abs_amax_clamp_div_mul_rel...         0.71%      20.969us         1.01%      29.590us      29.590us      12.992us         1.26%      12.992us      12.992us           0 b           0 b           0 b           0 b             1            --  \n",
      "                   triton_poi_fused__to_copy_mul_relu_2         0.66%      19.315us         0.91%      26.859us      26.859us       7.552us         0.73%       7.552us       7.552us           0 b           0 b           0 b           0 b             1            --  \n",
      "                               TorchDynamo Cache Lookup         1.35%      39.788us         1.35%      39.788us      39.788us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "                                         cuLaunchKernel         1.77%      52.048us         1.77%      52.048us      17.349us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             3            --  \n",
      "                                            aten::empty         0.65%      18.960us         0.65%      18.960us       9.480us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      16.00 Mb      16.00 Mb             2            --  \n",
      "                                             cudaMalloc         0.12%       3.478us         0.12%       3.478us       1.739us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                   cudaFuncSetAttribute         0.16%       4.762us         0.16%       4.762us       2.381us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                       cudaLaunchKernel         1.14%      33.346us         1.14%      33.346us      11.115us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             3            --  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.13%       3.722us         0.13%       3.722us       3.722us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     -16.00 Mb     -16.00 Mb             2            --  \n",
      "                                  cudaDeviceSynchronize        11.28%     331.150us        11.28%     331.150us     331.150us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.937ms\n",
      "Self CUDA time total: 1.030ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch profiler\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    "    profile_memory=True,\n",
    "    with_flops=True,\n",
    ") as prof:\n",
    "    output = ToyModel(input_data)\n",
    "    \n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6aaea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
