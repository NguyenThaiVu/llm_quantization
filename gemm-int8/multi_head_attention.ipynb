{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34da4e0b",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, we will implement the normal matmul and multi-head attention to check the correctness and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec789b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import gemm_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5107134",
   "metadata": {},
   "source": [
    "# 1. Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e07a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_row_int8_symmetric(mat: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric int8 quantization per row.\n",
    "    mat: (N, M) float tensor\n",
    "    Returns:\n",
    "      q_mat: (N, M) int8\n",
    "      scales: (N,) float32\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "    \n",
    "    max_vals = mat.abs().amax(dim=1, keepdim=True)  # (N, 1)\n",
    "    max_vals = max_vals.clamp(min=1e-8)\n",
    "\n",
    "    scales = (max_vals / qmax).squeeze(1)          # (N,)\n",
    "    q_mat = torch.clamp(torch.round(mat / scales.unsqueeze(1)), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    return q_mat, scales.to(torch.float32)\n",
    "\n",
    "\n",
    "def quantize_col_int8_symmetric(mat: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric int8 quantization per column.\n",
    "    mat: (N, M) float tensor\n",
    "    Returns:\n",
    "      q_mat: (N, M) int8\n",
    "      scales: (M,) float32\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "\n",
    "    max_vals = mat.abs().amax(dim=0, keepdim=True)  # (1, M)\n",
    "    max_vals = max_vals.clamp(min=1e-8)\n",
    "\n",
    "    scales = (max_vals / qmax).squeeze(0)           # (M,)\n",
    "    q_mat = torch.clamp(torch.round(mat / scales.unsqueeze(0)), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    return q_mat, scales.to(torch.float32)\n",
    "\n",
    "def quantize_tensor_int8_symmetric(tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric int8 quantization for entire tensor.\n",
    "    tensor: float tensor\n",
    "    Returns:\n",
    "      q_tensor: int8\n",
    "      scale: float32\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "\n",
    "    max_val = tensor.abs().amax()  # scalar\n",
    "    max_val = max_val.clamp(min=1e-8)\n",
    "\n",
    "    scale = (max_val / qmax)          # scalar\n",
    "    q_tensor = torch.clamp(torch.round(tensor / scale), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    return q_tensor, scale.to(torch.float32)\n",
    "\n",
    "def dequant_int8_gemm(out_int: torch.Tensor,\n",
    "                      x_scale: torch.Tensor,\n",
    "                      w_scale: torch.Tensor,\n",
    "                      out_dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    Dequantize result of INT8 matmul:\n",
    "      out_int: (B, out_features) int32 or float32\n",
    "      x_scale: (B,) from input rows\n",
    "      w_scale: (out_features,) from weight rows/cols\n",
    "    \"\"\"\n",
    "    if out_int.dtype == torch.int32:\n",
    "        out_float = out_int.to(torch.float32)\n",
    "    else:\n",
    "        out_float = out_int\n",
    "\n",
    "    # scale = x_scale.unsqueeze(1) * w_scale.unsqueeze(0)  # (B, out_features)\n",
    "    # out = out_float * scale\n",
    "    \n",
    "    out = out_float * x_scale[:, None] * w_scale[None, :]\n",
    "    \n",
    "    return out.to(out_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167f8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_func(func, *args, n_warmup=10, n_repeat=100):\n",
    "    torch._dynamo.reset()\n",
    "    # Warm-up\n",
    "    for _ in range(n_warmup):\n",
    "        func(*args)\n",
    "\n",
    "    # Benchmark\n",
    "    torch.cuda.synchronize()\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start_event.record()\n",
    "    for _ in range(n_repeat):\n",
    "        func(*args)\n",
    "    end_event.record()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "    avg_time_ms = elapsed_time_ms / n_repeat\n",
    "\n",
    "    return avg_time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36fef794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_true: torch.float16 torch.Size([4096, 4096])\n",
      "PyTorch matmul time: 0.417 ms\n"
     ]
    }
   ],
   "source": [
    "N = 4096\n",
    "M = 2048\n",
    "K = 4096\n",
    "device = 'cuda'\n",
    "d_type = torch.float16\n",
    "\n",
    "X = torch.randn(N, M, device=device, dtype=d_type)\n",
    "W = torch.randn(K, M, device=device, dtype=d_type)\n",
    "\n",
    "Y_true = torch.matmul(X, W.T)\n",
    "print(\"Y_true:\", Y_true.dtype, Y_true.shape)\n",
    "\n",
    "torch_time = benchmark_func(torch.matmul, X, W.T)\n",
    "print(f\"PyTorch matmul time: {torch_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba9bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb030108",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_q, X_scale = quantize_row_int8_symmetric(X)\n",
    "# X_q, X_scale = quantize_tensor_int8_symmetric(X)\n",
    "W_q, W_scale = quantize_row_int8_symmetric(W)\n",
    "\n",
    "if X_scale.numel() == 1:\n",
    "    X_scale = X_scale.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a91132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_deq: torch.float16 torch.Size([4096, 4096])\n",
      "INT8 matmul time: 0.277 ms\n"
     ]
    }
   ],
   "source": [
    "@torch.compile(dynamic=True)\n",
    "def gemm_int8_func(X_q, W_q, x_scale, w_scale):\n",
    "    \n",
    "    # if x_scale is not tensor\n",
    "    if type(x_scale) is float:\n",
    "        Y_int = gemm_int8.matmul(X_q, W_q, x_scale)\n",
    "        Y_deq = Y_int * w_scale.unsqueeze(0)\n",
    "    else:\n",
    "        Y_int = gemm_int8.matmul(X_q, W_q, 1.0)\n",
    "        Y_deq = dequant_int8_gemm(Y_int, x_scale, w_scale, out_dtype=d_type)\n",
    "    return Y_deq\n",
    "\n",
    "Y_deq = gemm_int8_func(X_q, W_q, X_scale, W_scale)\n",
    "print(\"Y_deq:\", Y_deq.dtype, Y_deq.shape)\n",
    "\n",
    "int8_time = benchmark_func(gemm_int8_func, X_q, W_q, X_scale, W_scale)\n",
    "print(f\"INT8 matmul time: {int8_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32db8b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed!\n"
     ]
    }
   ],
   "source": [
    "threshold = 3.0\n",
    "if torch.allclose(Y_true, Y_deq, atol=threshold, rtol=threshold):\n",
    "    print(\"passed!\")\n",
    "else:\n",
    "    print(\"================ INT8 matmul failed! ================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3b9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ea44095",
   "metadata": {},
   "source": [
    "# 2. Batched matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb0d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: torch.float16 torch.Size([8, 4096, 4096])\n",
      "PyTorch matmul time: 1.811 ms\n"
     ]
    }
   ],
   "source": [
    "H = 8\n",
    "D_k = 1024\n",
    "L = 4096\n",
    "\n",
    "Q = torch.randint(-127, 127, (H, L, D_k), device=device, dtype=torch.int8)\n",
    "K = torch.randint(-127, 127, (H, L, D_k), device=device, dtype=torch.int8)\n",
    "\n",
    "Q_fp = Q.to(torch.float16)\n",
    "K_fp = K.to(torch.float16)\n",
    "\n",
    "score = torch.matmul(Q_fp, K_fp.transpose(-2, -1)) \n",
    "print(\"score:\", score.dtype, score.shape)\n",
    "\n",
    "torch_time = benchmark_func(torch.matmul, Q_fp, K_fp.transpose(-2, -1))\n",
    "print(f\"PyTorch matmul time: {torch_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf754229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9477201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_int: torch.bfloat16 torch.Size([8, 4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "@torch.compile(dynamic=True)\n",
    "def gemm_batched_int8_func(X_q, W_q):\n",
    "    Y_int = gemm_int8.bmm_int8_matmul(X_q, W_q, 1.0)\n",
    "    return Y_int\n",
    "\n",
    "score_int = gemm_int8.bmm_int8_matmul(Q, K, 1.0)\n",
    "\n",
    "print(\"score_int:\", score_int.dtype, score_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b560b499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 batched matmul time: 1.401 ms\n"
     ]
    }
   ],
   "source": [
    "int8_time = benchmark_func(gemm_batched_int8_func, Q, K)\n",
    "print(f\"INT8 batched matmul time: {int8_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb370c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcae0b4c",
   "metadata": {},
   "source": [
    "# 3. Scale dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45b6dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dot_product(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Scaled dot-product attention.\n",
    "        Q: (H, L, D_k)\n",
    "        K: (H, L, D_k)\n",
    "        V: (H, L, D_v)\n",
    "        Returns:\n",
    "        out: (B, N, D_v)\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        scale_dk = torch.sqrt(torch.tensor(d_k, dtype=Q.dtype, device=Q.device))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / scale_dk  # (B, N, M)\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # (B, N, M)\n",
    "        out = torch.matmul(attn_weights, V)           # (B, N, D_v)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_true: torch.float16 torch.Size([8, 2048, 2048])\n",
      "PyTorch scaled dot-product attention time: 2.566 ms\n"
     ]
    }
   ],
   "source": [
    "L = 2048\n",
    "H = 8\n",
    "D_k = 2048\n",
    "\n",
    "Q = torch.randn(H, L, D_k, device=device, dtype=d_type)\n",
    "K = torch.randn(H, L, D_k, device=device, dtype=d_type)\n",
    "V = torch.randn(H, L, D_k, device=device, dtype=d_type)\n",
    "\n",
    "out_true = scale_dot_product(Q, K, V)\n",
    "print(\"out_true:\", out_true.dtype, out_true.shape)\n",
    "\n",
    "torch_scale_dot_product_time = benchmark_func(scale_dot_product, Q, K, V)\n",
    "print(f\"PyTorch scaled dot-product attention time: {torch_scale_dot_product_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943ac27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66310f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_row_matrix_int8_symmetric_batched(mat: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric per-row quantization for batched 3D tensor.\n",
    "    mat: [B, N, D]  (float tensor)\n",
    "    \n",
    "    Returns:\n",
    "        q_mat:   [B, N, D] int8\n",
    "        scales:  [B, N]    float32  (scale per row within each batch)\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "\n",
    "    # Compute max abs per row (per batch) - Result shape: [B, N, 1]\n",
    "    max_vals, _ = torch.max(torch.abs(mat), dim=2, keepdim=True)\n",
    "\n",
    "    # Compute scales per row\n",
    "    scales = (max_vals / qmax).clamp(min=1e-12)  # avoid div-by-zero, shape [B, N, 1]\n",
    "\n",
    "    # Quantize\n",
    "    q_mat = torch.clamp(torch.round(mat / scales), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    # Return float scales of shape [B, N]\n",
    "    scales = scales.squeeze(2).to(torch.float32)\n",
    "    return q_mat, scales      \n",
    "\n",
    "@torch.compile(dynamic=True)\n",
    "def scale_dot_product_int8(Q_q, Q_scale, K_q, K_scale, V_q, V_scale):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention with INT8 matmul.\n",
    "    Q: (H, L, D_k) int8\n",
    "    K: (H, L, D_k) int8\n",
    "    V: (H, L, D_v) float16\n",
    "    Returns:\n",
    "    out: (H, L, D_v) float16\n",
    "    \"\"\"\n",
    "    d_k = Q_q.size(-1)\n",
    "    scale_dk = torch.sqrt(torch.tensor(d_k, dtype=torch.float32, device=Q.device))\n",
    "    \n",
    "    scores_int = gemm_int8.bmm_int8_matmul(Q_q, K_q, 1.0)  # (H, L, L) int32\n",
    "    scores = Q_scale.unsqueeze(-1) * scores_int * K_scale.unsqueeze(1)\n",
    "    scores = scores / scale_dk   # (H, L, L) float32\n",
    "    scores = scores.to(torch.float16)\n",
    "\n",
    "    attn_weights = torch.softmax(scores, dim=-1)       # (H, L, L) float32\n",
    "    \n",
    "    attn_weights_q, attn_weights_scale = quantize_row_matrix_int8_symmetric_batched(attn_weights)\n",
    "    out_int = gemm_int8.bmm_int8_matmul(attn_weights_q, V_q)     # (H, L, D_v) float16\n",
    "    out = attn_weights_scale.unsqueeze(-1) * out_int * V_scale.unsqueeze(1)\n",
    "\n",
    "    return out.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "714be644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: torch.float16 torch.Size([8, 2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "Q_q, Q_scale = quantize_row_matrix_int8_symmetric_batched(Q)\n",
    "K_q, K_scale = quantize_row_matrix_int8_symmetric_batched(K)\n",
    "V_q, V_scale = quantize_row_matrix_int8_symmetric_batched(V)\n",
    "\n",
    "out = scale_dot_product_int8(Q_q, Q_scale, K_q, K_scale, V_q, V_scale)\n",
    "print(\"out:\", out.dtype, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85d62326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 scaled dot-product attention time: 1.902 ms\n"
     ]
    }
   ],
   "source": [
    "scale_dot_product_int8_time = benchmark_func(scale_dot_product_int8, Q_q, Q_scale, K_q, K_scale, V_q, V_scale)\n",
    "print(f\"INT8 scaled dot-product attention time: {scale_dot_product_int8_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ffd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "threshold = 2.0\n",
    "if torch.allclose(out_true, out, atol=threshold, rtol=threshold):\n",
    "    print(\"passed!\")\n",
    "else:\n",
    "    print(\"================ INT8 scaled dot-product attention failed! ================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbd8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba43b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd53d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
