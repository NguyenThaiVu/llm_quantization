{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68c0565",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, I will test the inference of GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035ad323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e7d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea509ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_BASE = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Original context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "PATH_MODEL = \"/scratch/tnguyen10/gpt2-xl-1558M.pth\"\n",
    "# PATH_MODEL = \"/scratch/tnguyen10/gpt2-medium-355M.pth\"\n",
    "model_name = \"gpt2-xl (1558M)\"  # FIX When changing model, update PATH_MODEL accordingly\n",
    "# model_name = \"gpt2-medium (355M)\"\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "NEW_CONFIG = GPT_CONFIG_BASE.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "model = GPTModel(NEW_CONFIG)\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(PATH_MODEL, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124af666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798df78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "What is the capital of the United States?\n",
      "\n",
      "The capital of the United States is Washington, D.C.\n",
      "\n",
      "What is the capital of the United Kingdom?\n",
      "\n",
      "The capital of the United Kingdom is London.\n",
      "\n",
      "What is the capital of Canada?\n",
      "\n",
      "The capital of Canada is Ottawa.\n",
      "\n",
      "What is the capital of Australia?\n",
      "\n",
      "The capital of Australia is Canberra.\n",
      "\n",
      "What is the capital of New Zealand?\n",
      "\n",
      "The capital of New Zealand is Wellington.\n",
      "\n",
      "What is the capital of South Africa?\n",
      "\n",
      "The capital of South Africa is Pretoria.\n",
      "\n",
      "What is the capital of India?\n",
      "\n",
      "The capital of India is New Delhi.\n",
      "\n",
      "What is the capital of Pakistan?\n",
      "\n",
      "The capital of Pakistan is Islamabad.\n",
      "\n",
      "What is the capital of Sri Lanka?\n",
      "\n",
      "The capital of Sri Lanka is Colombo.\n",
      "\n",
      "What is the capital of Nepal?\n",
      "\n",
      "The capital of Nepal is Kathmandu.\n",
      "\n",
      "What is the capital of Bangladesh?\n",
      "\n",
      "The capital of Bangladesh is Dhaka.\n",
      "\n",
      "What is the capital of Sri Lanka?\n",
      "\n",
      "The capital of Sri Lanka is Colombo.\n",
      "\n",
      "What is\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "enc_prompt = tokenizer.encode(prompt)\n",
    "enc_prompt = torch.tensor([enc_prompt])\n",
    "enc_prompt = enc_prompt.to(\"cuda\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=enc_prompt, \n",
    "    max_new_tokens=256, \n",
    "    context_size=NEW_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(token_ids.squeeze().tolist())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e9154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time over 5 runs: 9.3929 seconds\n",
      "----------\n",
      "What is the King of England?\n",
      "\n",
      "The King of England is the head of the British Commonwealth. He is the head of the British government and the head of the British military. He is the head of the British Parliament. He is the head of the British judiciary. He is the head of the British armed forces. He is the head of the British navy. He is the head of the British air force. He is the head of the British army. He is the head of the British Royal Navy. He is the head of the British Royal Air Force. He is the head of the British Royal Marines. He is the head of the British Royal Navy. He is the head of the British Royal Marines. He is the head of the British Royal Air Force. He is the head of the British Royal Navy. He is the head of the British Royal Air Force. He is the head of the British Royal Navy. He is the head of the British Royal Navy. He is the head of the British Royal Navy. He is the head of the British Royal Navy. He is the head of the British Royal Navy. He is the head of the British Royal Navy. He is the head of the British Royal Navy. He is the head of the British Royal Navy. He is the head of the British\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the King of England?\"\n",
    "enc_prompt = tokenizer.encode(prompt)\n",
    "enc_prompt = torch.tensor([enc_prompt])\n",
    "enc_prompt = enc_prompt.to(\"cuda\")\n",
    "\n",
    "# Measure inference time\n",
    "# Warm-up\n",
    "for _ in range(2):\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=enc_prompt, \n",
    "        max_new_tokens=256, \n",
    "        context_size=NEW_CONFIG[\"context_length\"]\n",
    "    )\n",
    "    \n",
    "n_iter = 5\n",
    "start_time = time.time()\n",
    "for _ in range(n_iter):\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=enc_prompt, \n",
    "        max_new_tokens=256, \n",
    "        context_size=NEW_CONFIG[\"context_length\"]\n",
    "    )\n",
    "end_time = time.time()\n",
    "avg_time = (end_time - start_time) / n_iter\n",
    "print(f\"Average inference time over {n_iter} runs: {avg_time:.4f} seconds\")\n",
    "print('-'*10)\n",
    "\n",
    "output = tokenizer.decode(token_ids.squeeze().tolist())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac94b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a44bbd47",
   "metadata": {},
   "source": [
    "# 2. Measure PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddd6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_ID = 50256  # gpt2's end token (not strictly needed here)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_ppl(model, tokenizer, texts, context_size, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Faster PPL: slide windows of length <= context_size and\n",
    "    score only the last token of each window (which has full left context).\n",
    "    \"\"\"\n",
    "    model_was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    for txt in texts:\n",
    "        ids = tokenizer.encode(txt)\n",
    "        if len(ids) < 2:\n",
    "            results.append({\"num_tokens\": 0, \"nll_sum\": 0.0, \"ppl\": float(\"nan\")})\n",
    "            continue\n",
    "\n",
    "        ids_t = torch.tensor(ids, dtype=torch.long, device=device)\n",
    "        nll_sum = 0.0\n",
    "        tok_cnt = 0\n",
    "\n",
    "        # We will take windows ending at positions end=1..L-1\n",
    "        L = ids_t.size(0)\n",
    "        end = 1\n",
    "        while end < L:\n",
    "            start = max(0, end - context_size)        # include up to token end-1\n",
    "            inp = ids_t[start:end].unsqueeze(0)       # [1, w] (predict token at 'end')\n",
    "            logits = model(inp)                        # [1, w, V]\n",
    "            last_logits = logits[:, -1, :]            # prediction for token at 'end'\n",
    "            target = ids_t[end].view(1)               # [1]\n",
    "            loss = F.cross_entropy(last_logits, target, reduction=\"sum\")\n",
    "            nll_sum += float(loss.item())\n",
    "            tok_cnt += 1\n",
    "\n",
    "            # Jump ahead by a stride: score roughly one token per window\n",
    "            # (Tune stride for speed/accuracy trade-off; 1 is exact; larger is faster.)\n",
    "            stride = max(1, context_size - 1)\n",
    "            end += stride\n",
    "\n",
    "        # If we skipped some tail tokens due to large stride, optionally finish them:\n",
    "        if end - (context_size - 1) < L - 1:\n",
    "            # exact tail sweep to ensure full coverage\n",
    "            for t in range(max(1, L - context_size + 1), L):\n",
    "                start = max(0, t - context_size)\n",
    "                inp = ids_t[start:t].unsqueeze(0)\n",
    "                logits = model(inp)\n",
    "                last_logits = logits[:, -1, :]\n",
    "                target = ids_t[t].view(1)\n",
    "                loss = F.cross_entropy(last_logits, target, reduction=\"sum\")\n",
    "                nll_sum += float(loss.item())\n",
    "                tok_cnt += 1\n",
    "\n",
    "        ppl = math.exp(nll_sum / max(tok_cnt, 1))\n",
    "        results.append({\"num_tokens\": tok_cnt, \"nll_sum\": nll_sum, \"ppl\": ppl})\n",
    "\n",
    "    total_nll = sum(r[\"nll_sum\"] for r in results)\n",
    "    total_tok = sum(r[\"num_tokens\"] for r in results) or 1\n",
    "    corpus_ppl = math.exp(total_nll / total_tok)\n",
    "\n",
    "    if model_was_training: model.train()\n",
    "    return results, corpus_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481461b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Load 1,000 samples from WikiText2 -----\n",
    "def load_wikitext2_samples(n=1000, min_length=10):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "    # Filter out empty or too-short lines\n",
    "    samples = [x[\"text\"] for x in dataset if len(x[\"text\"].strip()) > min_length]\n",
    "    return samples[:n]\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "samples = load_wikitext2_samples(num_samples)\n",
    "print(f\"Loaded {len(samples)} samples. Computing perplexity...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7153d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_text, corpus_ppl = compute_ppl(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,           \n",
    "    texts=samples,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# print(per_text)\n",
    "print(\"Corpus PPL:\", corpus_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbe04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a9806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe94aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
