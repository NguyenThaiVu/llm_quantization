{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68c0565",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, I will test the inference of GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ad323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import gemm_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb539249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_column_matrix_int_symmetric(mat:torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric quantization to int8 on a per-column basis.\n",
    "    mat: input float tensor (e.g., torch.float32 or torch.bfloat16)\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "    \n",
    "    max_vals, _ = torch.max(torch.abs(mat), dim=0, keepdim=True)  # shape (1, M)\n",
    "    scales = (max_vals / qmax).squeeze(0)  # shape (M,)\n",
    "    \n",
    "    q_mat = torch.clamp(torch.round(mat / scales.unsqueeze(0)), qmin, qmax).to(torch.int8)  # shape (N, M)\n",
    "    \n",
    "    scales = scales.clone().detach().to(torch.float32)\n",
    "    return q_mat, scales\n",
    "\n",
    "def quantize_row_int8_symmetric(mat: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric int8 quantization per row.\n",
    "    mat: (N, M) float tensor\n",
    "    Returns:\n",
    "      q_mat: (N, M) int8\n",
    "      scales: (N,) float32\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "    \n",
    "    max_vals = mat.abs().amax(dim=1, keepdim=True)  # (N, 1)\n",
    "    max_vals = max_vals.clamp(min=1e-8)\n",
    "\n",
    "    scales = (max_vals / qmax).squeeze(1)          # (N,)\n",
    "    q_mat = torch.clamp(torch.round(mat / scales.unsqueeze(1)), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    return q_mat, scales.to(torch.float32)\n",
    "\n",
    "\n",
    "def quantize_row_matrix_int8_symmetric_batched(mat: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric per-row quantization for batched 3D tensor.\n",
    "    mat: [B, N, D]  (float tensor)\n",
    "    \n",
    "    Returns:\n",
    "        q_mat:   [B, N, D] int8\n",
    "        scales:  [B, N]    float32  (scale per row within each batch)\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "\n",
    "    # Compute max abs per row (per batch) - Result shape: [B, N, 1]\n",
    "    max_vals, _ = torch.max(torch.abs(mat), dim=2, keepdim=True)\n",
    "\n",
    "    # Compute scales per row\n",
    "    scales = (max_vals / qmax).clamp(min=1e-12)  # avoid div-by-zero, shape [B, N, 1]\n",
    "\n",
    "    # Quantize\n",
    "    q_mat = torch.clamp(torch.round(mat / scales), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    # Return float scales of shape [B, N]\n",
    "    scales = scales.squeeze(2).to(torch.float32)\n",
    "    return q_mat, scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ba1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        # Quantization parameters\n",
    "        self.register_buffer(\"weight_q\", torch.empty_like(self.weight, dtype=torch.int8), persistent=False)\n",
    "        self.register_buffer(\"weight_scale\", torch.empty(out_features, dtype=torch.float32), persistent=False)\n",
    "        self.is_quantized = False\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def quantize_weights(self):\n",
    "        weight_q, weight_scale = quantize_row_int8_symmetric(self.weight)\n",
    "        self.weight_q.copy_(weight_q)\n",
    "        self.weight_scale.copy_(weight_scale)\n",
    "        print(f\"[INFO] Done quantize linear layer weights - shape {self.weight_q.shape}\")\n",
    "        self.is_quantized = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_quantized == False:\n",
    "            y = torch.matmul(x, self.weight.t())\n",
    "            if self.bias is not None:\n",
    "                y = y + self.bias\n",
    "        else:\n",
    "            x_q, x_scale = quantize_row_matrix_int8_symmetric_batched(x)\n",
    "            y_int = gemm_int8.bmm_int8_matmul(x_q, self.weight_q)\n",
    "            y = x_scale.unsqueeze(-1) * y_int * self.weight_scale[None, :]\n",
    "            \n",
    "            y = y.to(x.dtype)\n",
    "            if self.bias is not None:\n",
    "                y = y + self.bias\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        # self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_query = CustomLinear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = CustomLinear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = CustomLinear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            CustomLinear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            CustomLinear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea509ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_BASE = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Original context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "PATH_MODEL = \"/scratch/tnguyen10/gpt2-xl-1558M.pth\"\n",
    "# PATH_MODEL = \"/scratch/tnguyen10/gpt2-medium-355M.pth\"\n",
    "model_name = \"gpt2-xl (1558M)\"  # FIX When changing model, update PATH_MODEL accordingly\n",
    "# model_name = \"gpt2-medium (355M)\"\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "NEW_CONFIG = GPT_CONFIG_BASE.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "model = GPTModel(NEW_CONFIG).to(DEVICE).eval()\n",
    "# model = torch.compile(model, dynamic=True)\n",
    "\n",
    "model.load_state_dict(torch.load(PATH_MODEL, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124af666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d3753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29059390",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "enc_prompt = tokenizer.encode(prompt)\n",
    "enc_prompt = torch.tensor([enc_prompt])\n",
    "enc_prompt = enc_prompt.to(\"cuda\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=enc_prompt, \n",
    "    max_new_tokens=256, \n",
    "    context_size=NEW_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(token_ids.squeeze().tolist())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798df78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the King of England?\"\n",
    "enc_prompt = tokenizer.encode(prompt)\n",
    "enc_prompt = torch.tensor([enc_prompt])\n",
    "enc_prompt = enc_prompt.to(\"cuda\")\n",
    "\n",
    "# Measure inference time\n",
    "# Warm-up\n",
    "for _ in range(2):\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=enc_prompt, \n",
    "        max_new_tokens=256, \n",
    "        context_size=NEW_CONFIG[\"context_length\"]\n",
    "    )\n",
    "    \n",
    "n_iter = 5\n",
    "start_time = time.time()\n",
    "for _ in range(n_iter):\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=enc_prompt, \n",
    "        max_new_tokens=256, \n",
    "        context_size=NEW_CONFIG[\"context_length\"]\n",
    "    )\n",
    "end_time = time.time()\n",
    "avg_time = (end_time - start_time) / n_iter\n",
    "print(f\"Average inference time over {n_iter} runs: {avg_time:.4f} seconds\")\n",
    "print('-'*10)\n",
    "\n",
    "output = tokenizer.decode(token_ids.squeeze().tolist())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49169c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f90cb38",
   "metadata": {},
   "source": [
    "# 2. Quantize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, CustomLinear):\n",
    "        print(f\"\\nQuantize weights module: {name} ...\")\n",
    "        module.quantize_weights()\n",
    "        print(f\"Quantizate done : {name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "enc_prompt = tokenizer.encode(prompt)\n",
    "enc_prompt = torch.tensor([enc_prompt])\n",
    "enc_prompt = enc_prompt.to(\"cuda\")\n",
    "\n",
    "# Measure inference time after quantization\n",
    "# Warm-up\n",
    "for _ in range(2):\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=enc_prompt, \n",
    "        max_new_tokens=256, \n",
    "        context_size=NEW_CONFIG[\"context_length\"]\n",
    "    )\n",
    "    \n",
    "n_iter = 5\n",
    "start_time = time.time()\n",
    "for _ in range(n_iter):\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=enc_prompt, \n",
    "        max_new_tokens=100, \n",
    "        context_size=NEW_CONFIG[\"context_length\"]\n",
    "    )\n",
    "end_time = time.time()\n",
    "avg_time = (end_time - start_time) / n_iter\n",
    "print(f\"Average inference time after quantization over {n_iter} runs: {avg_time:.4f} seconds\")\n",
    "print('-'*10)   \n",
    "\n",
    "output = tokenizer.decode(token_ids.squeeze().tolist())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the King of England?\"\n",
    "enc_prompt = tokenizer.encode(prompt)\n",
    "enc_prompt = torch.tensor([enc_prompt])\n",
    "enc_prompt = enc_prompt.to(\"cuda\")\n",
    "\n",
    "# Measure inference time\n",
    "# Warm-up\n",
    "for _ in range(2):\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=enc_prompt, \n",
    "        max_new_tokens=256, \n",
    "        context_size=NEW_CONFIG[\"context_length\"]\n",
    "    )\n",
    "    \n",
    "n_iter = 5\n",
    "start_time = time.time()\n",
    "for _ in range(n_iter):\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=enc_prompt, \n",
    "        max_new_tokens=256, \n",
    "        context_size=NEW_CONFIG[\"context_length\"]\n",
    "    )\n",
    "end_time = time.time()\n",
    "avg_time = (end_time - start_time) / n_iter\n",
    "print(f\"Average inference time over {n_iter} runs: {avg_time:.4f} seconds\")\n",
    "print('-'*10)\n",
    "\n",
    "output = tokenizer.decode(token_ids.squeeze().tolist())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac94b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a44bbd47",
   "metadata": {},
   "source": [
    "# 2. Measure PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddd6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_ID = 50256  # gpt2's end token (not strictly needed here)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_ppl(model, tokenizer, texts, context_size, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Faster PPL: slide windows of length <= context_size and\n",
    "    score only the last token of each window (which has full left context).\n",
    "    \"\"\"\n",
    "    model_was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    for txt in texts:\n",
    "        ids = tokenizer.encode(txt)\n",
    "        if len(ids) < 2:\n",
    "            results.append({\"num_tokens\": 0, \"nll_sum\": 0.0, \"ppl\": float(\"nan\")})\n",
    "            continue\n",
    "\n",
    "        ids_t = torch.tensor(ids, dtype=torch.long, device=device)\n",
    "        nll_sum = 0.0\n",
    "        tok_cnt = 0\n",
    "\n",
    "        # We will take windows ending at positions end=1..L-1\n",
    "        L = ids_t.size(0)\n",
    "        end = 1\n",
    "        while end < L:\n",
    "            start = max(0, end - context_size)        # include up to token end-1\n",
    "            inp = ids_t[start:end].unsqueeze(0)       # [1, w] (predict token at 'end')\n",
    "            logits = model(inp)                        # [1, w, V]\n",
    "            last_logits = logits[:, -1, :]            # prediction for token at 'end'\n",
    "            target = ids_t[end].view(1)               # [1]\n",
    "            loss = F.cross_entropy(last_logits, target, reduction=\"sum\")\n",
    "            nll_sum += float(loss.item())\n",
    "            tok_cnt += 1\n",
    "\n",
    "            # Jump ahead by a stride: score roughly one token per window\n",
    "            # (Tune stride for speed/accuracy trade-off; 1 is exact; larger is faster.)\n",
    "            stride = max(1, context_size - 1)\n",
    "            end += stride\n",
    "\n",
    "        # If we skipped some tail tokens due to large stride, optionally finish them:\n",
    "        if end - (context_size - 1) < L - 1:\n",
    "            # exact tail sweep to ensure full coverage\n",
    "            for t in range(max(1, L - context_size + 1), L):\n",
    "                start = max(0, t - context_size)\n",
    "                inp = ids_t[start:t].unsqueeze(0)\n",
    "                logits = model(inp)\n",
    "                last_logits = logits[:, -1, :]\n",
    "                target = ids_t[t].view(1)\n",
    "                loss = F.cross_entropy(last_logits, target, reduction=\"sum\")\n",
    "                nll_sum += float(loss.item())\n",
    "                tok_cnt += 1\n",
    "\n",
    "        ppl = math.exp(nll_sum / max(tok_cnt, 1))\n",
    "        results.append({\"num_tokens\": tok_cnt, \"nll_sum\": nll_sum, \"ppl\": ppl})\n",
    "\n",
    "    total_nll = sum(r[\"nll_sum\"] for r in results)\n",
    "    total_tok = sum(r[\"num_tokens\"] for r in results) or 1\n",
    "    corpus_ppl = math.exp(total_nll / total_tok)\n",
    "\n",
    "    if model_was_training: model.train()\n",
    "    return results, corpus_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481461b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Load 1,000 samples from WikiText2 -----\n",
    "def load_wikitext2_samples(n=1000, min_length=10):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "    # Filter out empty or too-short lines\n",
    "    samples = [x[\"text\"] for x in dataset if len(x[\"text\"].strip()) > min_length]\n",
    "    return samples[:n]\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "samples = load_wikitext2_samples(num_samples)\n",
    "print(f\"Loaded {len(samples)} samples. Computing perplexity...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7153d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_text, corpus_ppl = compute_ppl(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,           \n",
    "    texts=samples,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# print(per_text)\n",
    "print(\"Corpus PPL:\", corpus_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbe04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a9806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe94aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
