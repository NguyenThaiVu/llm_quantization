{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efde77f2-6af3-4781-8597-89ecd3f41a52",
   "metadata": {
    "id": "efde77f2-6af3-4781-8597-89ecd3f41a52"
   },
   "source": [
    "# Qwen3 From Scratch (A Standalone Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed2b109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 0.35.0\n",
      "tokenizers version: 0.22.1\n",
      "torch version: 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "torch.random.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "from importlib.metadata import version\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import int8_linear_matmul\n",
    "import time\n",
    "import gemm_int8\n",
    "\n",
    "pkgs = [\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tokenizers\",       # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a90338-624a-4706-aa55-6b4358070194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which model to use via the following flag; only one can be True\n",
    "USE_BASE_MODEL = True\n",
    "USE_REASONING_MODEL = False\n",
    "USE_INSTRUCT_MODEL = False\n",
    "\n",
    "CHOOSE_MODEL = \"8B\"  # Options: \"0.6B\", \"1.7B\" \"4B\", \"8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653410a6-dd2b-4eb2-a722-23d9782e726d",
   "metadata": {
    "id": "653410a6-dd2b-4eb2-a722-23d9782e726d"
   },
   "source": [
    "&nbsp;\n",
    "# 1. Architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2f5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def quantized_column_matrix_int_symmetric(mat:torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric quantization to int8 on a per-column basis.\n",
    "    mat: input float tensor (e.g., torch.float32 or torch.bfloat16)\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "    \n",
    "    max_vals, _ = torch.max(torch.abs(mat), dim=0, keepdim=True)  # shape (1, M)\n",
    "    scales = (max_vals / qmax).squeeze(0)  # shape (M,)\n",
    "    \n",
    "    q_mat = torch.clamp(torch.round(mat / scales.unsqueeze(0)), qmin, qmax).to(torch.int8)  # shape (N, M)\n",
    "    \n",
    "    scales = scales.clone().detach().to(torch.float32)\n",
    "    return q_mat, scales\n",
    "\n",
    "def quantize_row_int8_symmetric(mat: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric int8 quantization per row.\n",
    "    mat: (N, M) float tensor\n",
    "    Returns:\n",
    "      q_mat: (N, M) int8\n",
    "      scales: (N,) float32\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "    \n",
    "    max_vals = mat.abs().amax(dim=1, keepdim=True)  # (N, 1)\n",
    "    max_vals = max_vals.clamp(min=1e-8)\n",
    "\n",
    "    scales = (max_vals / qmax).squeeze(1)          # (N,)\n",
    "    q_mat = torch.clamp(torch.round(mat / scales.unsqueeze(1)), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    return q_mat, scales.to(torch.float32)\n",
    "\n",
    "\n",
    "def quantize_row_matrix_int8_symmetric_batched(mat: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric per-row quantization for batched 3D tensor.\n",
    "    mat: [B, N, D]  (float tensor)\n",
    "    \n",
    "    Returns:\n",
    "        q_mat:   [B, N, D] int8\n",
    "        scales:  [B, N]    float32  (scale per row within each batch)\n",
    "    \"\"\"\n",
    "    qmin, qmax = -128, 127\n",
    "\n",
    "    # Compute max abs per row (per batch) - Result shape: [B, N, 1]\n",
    "    max_vals, _ = torch.max(torch.abs(mat), dim=2, keepdim=True)\n",
    "\n",
    "    # Compute scales per row\n",
    "    scales = (max_vals / qmax).clamp(min=1e-12)  # avoid div-by-zero, shape [B, N, 1]\n",
    "\n",
    "    # Quantize\n",
    "    q_mat = torch.clamp(torch.round(mat / scales), qmin, qmax).to(torch.int8)\n",
    "\n",
    "    # Return float scales of shape [B, N]\n",
    "    scales = scales.squeeze(2).to(torch.float32)\n",
    "    return q_mat, scales\n",
    "\n",
    "module_quantize_row_matrix_int8_symmetric_batched = torch.compile(quantize_row_matrix_int8_symmetric_batched)\n",
    "\n",
    "\n",
    "\n",
    "def int8_and_dequantize(x_q, w_q, x_scales, w_scales, output_dtype=torch.bfloat16):\n",
    "    \"\"\"\n",
    "    x_q: (B, N, D) int8\n",
    "    w_q: (M, D) int8\n",
    "    x_scales: (B, N) float32\n",
    "    w_scales: (M,) float32\n",
    "    Returns:\n",
    "      y: (B, N, M) float32\n",
    "    \"\"\"\n",
    "    y_int = int8_linear_matmul(x_q, w_q)  # (B, N, M) int32\n",
    "    y = x_scales.unsqueeze(-1) * y_int * w_scales[None, :]\n",
    "    return y.to(output_dtype)\n",
    "\n",
    "module_int8_and_dequantize = torch.compile(int8_and_dequantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f15cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features, dtype=dtype))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_features, dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        # Quantization parameters\n",
    "        self.register_buffer(\"weight_q\", torch.empty_like(self.weight, dtype=torch.int8), persistent=False)\n",
    "        self.register_buffer(\"weight_scale\", torch.empty(out_features, dtype=torch.float32), persistent=False)\n",
    "        self.is_quantized = False\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"CustomLinear(in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]})\"\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def quantize_weights(self):\n",
    "        weight_q, weight_scale = quantize_row_int8_symmetric(self.weight)\n",
    "        self.weight_q.copy_(weight_q)\n",
    "        self.weight_scale.copy_(weight_scale)\n",
    "        print(f\"[INFO] Done quantize linear layer weights - shape {self.weight_q.shape}\")\n",
    "        self.is_quantized = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_quantized == False:\n",
    "            y = torch.matmul(x, self.weight.t())\n",
    "            if self.bias is not None:\n",
    "                y = y + self.bias\n",
    "        else:\n",
    "            x_q, x_scale = module_quantize_row_matrix_int8_symmetric_batched(x)\n",
    "            # x_q, x_scale = gemm_int8.batched_quantize_row_int8_symmetric(x)\n",
    "            \n",
    "            y = gemm_int8.int8_bmm_matmul_and_quantize(\n",
    "                x_q, \n",
    "                self.weight_q, \n",
    "                1.0, \n",
    "                x_scale, \n",
    "                self.weight_scale)\n",
    "\n",
    "            if self.bias is not None:\n",
    "                y = y + self.bias\n",
    "        return y.to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82076c21-9331-4dcd-b017-42b046cf1a60",
   "metadata": {
    "id": "82076c21-9331-4dcd-b017-42b046cf1a60"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        # self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        # self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        \n",
    "        self.fc1 = CustomLinear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = CustomLinear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = CustomLinear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56715760-37e1-433e-89da-04864c139a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.qwen3_compatible = qwen3_compatible\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "\n",
    "        if self.qwen3_compatible:\n",
    "            x = x.to(torch.float32)\n",
    "\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
    "        norm_x = norm_x * self.scale\n",
    "\n",
    "        if self.shift is not None:\n",
    "            norm_x = norm_x + self.shift\n",
    "\n",
    "        return norm_x.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9a346f-5826-4083-9162-abd56afc03f0",
   "metadata": {
    "id": "4b9a346f-5826-4083-9162-abd56afc03f0"
   },
   "outputs": [],
   "source": [
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb",
   "metadata": {
    "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb"
   },
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        # self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_query = CustomLinear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        \n",
    "        # self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_key = CustomLinear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        \n",
    "        # self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = CustomLinear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        \n",
    "        # self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "        self.out_proj = CustomLinear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Apply projections\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Optional normalization\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys = self.k_norm(keys)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin)\n",
    "        keys = apply_rope(keys, cos, sin)\n",
    "\n",
    "        # Expand K and V to match number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9",
   "metadata": {
    "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x, mask, cos, sin)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4",
   "metadata": {
    "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4"
   },
   "outputs": [],
   "source": [
    "class Qwen3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        # Reusuable utilities\n",
    "        if cfg[\"head_dim\"] is None:\n",
    "            head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
    "        else:\n",
    "            head_dim = cfg[\"head_dim\"]\n",
    "        cos, sin = compute_rope_params(\n",
    "            head_dim=head_dim,\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"]\n",
    "        )\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.cfg = cfg\n",
    "\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # Forward pass\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "\n",
    "        num_tokens = x.shape[1]\n",
    "        mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        \n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x, mask, self.cos, self.sin)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d201f-74ad-4d63-ab9c-601b00674a48",
   "metadata": {
    "id": "be2d201f-74ad-4d63-ab9c-601b00674a48"
   },
   "source": [
    "&nbsp;\n",
    "# 2. Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caa142fa-b375-4e78-b392-2072ced666f3",
   "metadata": {
    "id": "caa142fa-b375-4e78-b392-2072ced666f3"
   },
   "outputs": [],
   "source": [
    "if CHOOSE_MODEL == \"0.6B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,           # Vocabulary size\n",
    "        \"context_length\": 40_960,        # Context length that was used to train the model\n",
    "        \"emb_dim\": 1024,                 # Embedding dimension\n",
    "        \"n_heads\": 16,                   # Number of attention heads\n",
    "        \"n_layers\": 28,                  # Number of layers\n",
    "        \"hidden_dim\": 3072,              # Size of the intermediate dimension in FeedForward\n",
    "        \"head_dim\": 128,                 # Size of the heads in GQA\n",
    "        \"qk_norm\": True,                 # Whether to normalize queries and keys in GQA\n",
    "        \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
    "        \"rope_base\": 1_000_000.0,        # The base in RoPE's \"theta\"\n",
    "        \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
    "    }\n",
    "\n",
    "elif CHOOSE_MODEL == \"1.7B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 2048,                 # 2x larger than above\n",
    "        \"n_heads\": 16,\n",
    "        \"n_layers\": 28,\n",
    "        \"hidden_dim\": 6144,              # 2x larger than above\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }   \n",
    "\n",
    "elif CHOOSE_MODEL == \"4B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 2560,                 # 25% larger than above\n",
    "        \"n_heads\": 32,                   # 2x larger than above\n",
    "        \"n_layers\": 36,                  # 29% larger than above\n",
    "        \"hidden_dim\": 9728,              # ~3x larger than above\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    }  \n",
    "\n",
    "elif CHOOSE_MODEL == \"8B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 4096,                 # 60% larger than above\n",
    "        \"n_heads\": 32,\n",
    "        \"n_layers\": 36,                  # 26% larger than above\n",
    "        \"hidden_dim\": 12288,\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    } \n",
    "\n",
    "elif CHOOSE_MODEL == \"14B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 5120,                 # 25% larger than above\n",
    "        \"n_heads\": 40,                   # 25% larger than above\n",
    "        \"n_layers\": 40,                  # 11% larger than above\n",
    "        \"hidden_dim\": 17408,             # 42% larger than above\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    } \n",
    "\n",
    "elif CHOOSE_MODEL == \"32B\":\n",
    "    QWEN3_CONFIG = {\n",
    "        \"vocab_size\": 151_936,\n",
    "        \"context_length\": 40_960,\n",
    "        \"emb_dim\": 5120,                \n",
    "        \"n_heads\": 64,                   # 60% larger than above\n",
    "        \"n_layers\": 64,                  # 60% larger than above\n",
    "        \"hidden_dim\": 25600,             # 47% larger than above\n",
    "        \"head_dim\": 128,\n",
    "        \"qk_norm\": True,\n",
    "        \"n_kv_groups\": 8,\n",
    "        \"rope_base\": 1_000_000.0,\n",
    "        \"dtype\": torch.bfloat16,\n",
    "    } \n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"{CHOOSE_MODEL} is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "156253fe-aacd-4da2-8f13-705f05c4b11e",
   "metadata": {
    "id": "156253fe-aacd-4da2-8f13-705f05c4b11e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (tok_emb): Embedding(151936, 4096)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-35): 36 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): CustomLinear(in_features=4096, out_features=4096)\n",
       "        (W_key): CustomLinear(in_features=4096, out_features=1024)\n",
       "        (W_value): CustomLinear(in_features=4096, out_features=1024)\n",
       "        (out_proj): CustomLinear(in_features=4096, out_features=4096)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): CustomLinear(in_features=4096, out_features=12288)\n",
       "        (fc2): CustomLinear(in_features=4096, out_features=12288)\n",
       "        (fc3): CustomLinear(in_features=12288, out_features=4096)\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = Qwen3Model(QWEN3_CONFIG)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
    "outputId": "00d7e983-262e-4c65-f322-f4d999311988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 8,190,735,360\n",
      "\n",
      "Total number of unique parameters: 7,568,405,504\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Account for weight tying\n",
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
    "outputId": "65c1a95e-b502-4150-9e2e-da619d9053d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 86.95 GB\n",
      "bfloat16: 43.47 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f12baf-f79b-499f-85c0-51328a6a20f5",
   "metadata": {
    "id": "31f12baf-f79b-499f-85c0-51328a6a20f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172f89f-d301-439f-b809-46169e5f5945",
   "metadata": {
    "id": "c172f89f-d301-439f-b809-46169e5f5945"
   },
   "source": [
    "# 3. Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75166128-5899-4995-9b88-9672e135650e",
   "metadata": {
    "id": "75166128-5899-4995-9b88-9672e135650e"
   },
   "outputs": [],
   "source": [
    "def load_weights_into_qwen(model, param_config, params):\n",
    "    def assign(left, right, tensor_name=\"unknown\"):\n",
    "        if left.shape != right.shape:\n",
    "            raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if isinstance(right, torch.Tensor):\n",
    "                left.copy_(right)\n",
    "            else:\n",
    "                left.copy_(torch.as_tensor(right, dtype=left.dtype, device=left.device))\n",
    "    \n",
    "        return left \n",
    "\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "        block = model.trf_blocks[l]\n",
    "        att = block.att\n",
    "\n",
    "        # Q, K, V projections\n",
    "        att.W_query.weight = assign(\n",
    "            att.W_query.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
    "        )\n",
    "        att.W_key.weight = assign(\n",
    "            att.W_key.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
    "        )\n",
    "        att.W_value.weight = assign(\n",
    "            att.W_value.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        att.out_proj.weight = assign(\n",
    "            att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
    "        )\n",
    "\n",
    "        # QK norms\n",
    "        if hasattr(att, \"q_norm\") and att.q_norm is not None:\n",
    "            att.q_norm.scale = assign(\n",
    "                att.q_norm.scale,\n",
    "                params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
    "                f\"model.layers.{l}.self_attn.q_norm.weight\"\n",
    "            )\n",
    "        if hasattr(att, \"k_norm\") and att.k_norm is not None:\n",
    "            att.k_norm.scale = assign(\n",
    "                att.k_norm.scale,\n",
    "                params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
    "                f\"model.layers.{l}.self_attn.k_norm.weight\"\n",
    "            )\n",
    "\n",
    "        # Attention layernorm\n",
    "        block.norm1.scale = assign(\n",
    "            block.norm1.scale,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.input_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "        # Feedforward weights\n",
    "        block.ff.fc1.weight = assign(\n",
    "            block.ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
    "        )\n",
    "        block.ff.fc2.weight = assign(\n",
    "            block.ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
    "        )\n",
    "        block.ff.fc3.weight = assign(\n",
    "            block.ff.fc3.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
    "        )\n",
    "        block.norm2.scale = assign(\n",
    "            block.norm2.scale,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "    # Final normalization and output head\n",
    "    model.final_norm.scale = assign(model.final_norm.scale, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
    "\n",
    "    if \"lm_head.weight\" in params:\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
    "    else:\n",
    "        model.out_head.weight = model.tok_emb.weight\n",
    "        print(\"Model uses weight tying.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9881b6995c3f49dc89e6992fd9ab660b",
      "17a3174e65c54476b2e0d1faf8f011ca",
      "1bbf2e62c0754d1593beb4105a7f1ac1",
      "b82112e1dec645d98aa1c1ba64abcb61",
      "271e2bd6a35e4a8b92de8697f7c0be5f",
      "90a79523187446dfa692723b2e5833a7",
      "431ffb83b8c14bf182f0430e07ea6154",
      "a8f1b72a33dd4b548de23fbd95e0da18",
      "25cc36132d384189acfbecc59483134b",
      "bfd06423ad544218968648016e731a46",
      "d029630b63ff44cf807ade428d2eb421"
     ]
    },
    "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
    "outputId": "55b2f28c-142f-4698-9d23-d27456d3ed6d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97694a271a4642d7b0b4f52937768711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "\n",
    "if USE_REASONING_MODEL or USE_INSTRUCT_MODEL:\n",
    "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}\"\n",
    "else:\n",
    "    repo_id = f\"Qwen/Qwen3-{CHOOSE_MODEL}-Base\"\n",
    "\n",
    "MODEL_HUD_FOLDER = \"/scratch/tnguyen10/\"\n",
    "\n",
    "local_dir = Path(repo_id).parts[-1]\n",
    "local_dir = os.path.join(MODEL_HUD_FOLDER, local_dir)\n",
    "\n",
    "if CHOOSE_MODEL == \"0.6B\":\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=\"model.safetensors\",\n",
    "        local_dir=local_dir,\n",
    "    )\n",
    "    weights_dict = load_file(weights_file)\n",
    "else:\n",
    "    repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
    "    index_path = os.path.join(repo_dir, \"model.safetensors.index.json\")\n",
    "    with open(index_path, \"r\") as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    weights_dict = {}\n",
    "    for filename in set(index[\"weight_map\"].values()):\n",
    "        shard_path = os.path.join(repo_dir, filename)\n",
    "        shard = load_file(shard_path)\n",
    "        weights_dict.update(shard)\n",
    "\n",
    "load_weights_into_qwen(model, QWEN3_CONFIG, weights_dict)\n",
    "model.to(device)\n",
    "del weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef8a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc63512",
   "metadata": {},
   "source": [
    "## 3.1. Quantization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca66b104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantize weights module: trf_blocks.0.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.0.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.0.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.0.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.0.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.0.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.0.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.0.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.0.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.0.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.0.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.0.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.0.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.0.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.1.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.1.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.1.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.1.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.1.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.1.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.1.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.1.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.1.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.1.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.1.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.1.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.1.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.1.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.2.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.2.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.2.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.2.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.2.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.2.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.2.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.2.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.2.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.2.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.2.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.2.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.2.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.2.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.3.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.3.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.3.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.3.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.3.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.3.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.3.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.3.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.3.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.3.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.3.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.3.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.3.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.3.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.4.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.4.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.4.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.4.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.4.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.4.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.4.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.4.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.4.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.4.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.4.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.4.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.4.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.4.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.5.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.5.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.5.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.5.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.5.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.5.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.5.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.5.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.5.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.5.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.5.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.5.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.5.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.5.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.6.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.6.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.6.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.6.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.6.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.6.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.6.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.6.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.6.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.6.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.6.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.6.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.6.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.6.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.7.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.7.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.7.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.7.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.7.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.7.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.7.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.7.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.7.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.7.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.7.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.7.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.7.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.7.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.8.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.8.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.8.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.8.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.8.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.8.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.8.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.8.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.8.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.8.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.8.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.8.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.8.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.8.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.9.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.9.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.9.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.9.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.9.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.9.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.9.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.9.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.9.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.9.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.9.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.9.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.9.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.9.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.10.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.10.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.10.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.10.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.10.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.10.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.10.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.10.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.10.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.10.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.10.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.10.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.10.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.10.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.11.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.11.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.11.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.11.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.11.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.11.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.11.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.11.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.11.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.11.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.11.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.11.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.11.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.11.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.12.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.12.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.12.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.12.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.12.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.12.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.12.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.12.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.12.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.12.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.12.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.12.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.12.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.12.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.13.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.13.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.13.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.13.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.13.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.13.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.13.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.13.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.13.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.13.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.13.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.13.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.13.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.13.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.14.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.14.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.14.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.14.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.14.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.14.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.14.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.14.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.14.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.14.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.14.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.14.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.14.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.14.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.15.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.15.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.15.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.15.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.15.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.15.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.15.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.15.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.15.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.15.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.15.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.15.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.15.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.15.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.16.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.16.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.16.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.16.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.16.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.16.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.16.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.16.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.16.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.16.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.16.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.16.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.16.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.16.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.17.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.17.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.17.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.17.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.17.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.17.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.17.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.17.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.17.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.17.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.17.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.17.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.17.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.17.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.18.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.18.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.18.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.18.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.18.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.18.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.18.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.18.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.18.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.18.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.18.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.18.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.18.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.18.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.19.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.19.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.19.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.19.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.19.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.19.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.19.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.19.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.19.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.19.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.19.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.19.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.19.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.19.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.20.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.20.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.20.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.20.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.20.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.20.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.20.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.20.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.20.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.20.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.20.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.20.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.20.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.20.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.21.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.21.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.21.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.21.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.21.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.21.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.21.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.21.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.21.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.21.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.21.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.21.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.21.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.21.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.22.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.22.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.22.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.22.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.22.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.22.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.22.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.22.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.22.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.22.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.22.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.22.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.22.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.22.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.23.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.23.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.23.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.23.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.23.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.23.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.23.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.23.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.23.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.23.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.23.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.23.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.23.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.23.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.24.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.24.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.24.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.24.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.24.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.24.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.24.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.24.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.24.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.24.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.24.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.24.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.24.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.24.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.25.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.25.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.25.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.25.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.25.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.25.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.25.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.25.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.25.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.25.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.25.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.25.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.25.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.25.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.26.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.26.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.26.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.26.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.26.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.26.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.26.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.26.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.26.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.26.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.26.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.26.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.26.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.26.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.27.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.27.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.27.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.27.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.27.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.27.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.27.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.27.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.27.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.27.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.27.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.27.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.27.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.27.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.28.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.28.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.28.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.28.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.28.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.28.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.28.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.28.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.28.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.28.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.28.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.28.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.28.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.28.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.29.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.29.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.29.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.29.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.29.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.29.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.29.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.29.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.29.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.29.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.29.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.29.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.29.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.29.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.30.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.30.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.30.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.30.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.30.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.30.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.30.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.30.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.30.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.30.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.30.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.30.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.30.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.30.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.31.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.31.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.31.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.31.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.31.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.31.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.31.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.31.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.31.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.31.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.31.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.31.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.31.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.31.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.32.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.32.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.32.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.32.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.32.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.32.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.32.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.32.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.32.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.32.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.32.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.32.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.32.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.32.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.33.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.33.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.33.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.33.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.33.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.33.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.33.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.33.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.33.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.33.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.33.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.33.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.33.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.33.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.34.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.34.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.34.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.34.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.34.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.34.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.34.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.34.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.34.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.34.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.34.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.34.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.34.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.34.ff.fc3.\n",
      "\n",
      "Quantize weights module: trf_blocks.35.att.W_query ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.35.att.W_query.\n",
      "\n",
      "Quantize weights module: trf_blocks.35.att.W_key ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.35.att.W_key.\n",
      "\n",
      "Quantize weights module: trf_blocks.35.att.W_value ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([1024, 4096])\n",
      "Quantizate done : trf_blocks.35.att.W_value.\n",
      "\n",
      "Quantize weights module: trf_blocks.35.att.out_proj ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 4096])\n",
      "Quantizate done : trf_blocks.35.att.out_proj.\n",
      "\n",
      "Quantize weights module: trf_blocks.35.ff.fc1 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.35.ff.fc1.\n",
      "\n",
      "Quantize weights module: trf_blocks.35.ff.fc2 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([12288, 4096])\n",
      "Quantizate done : trf_blocks.35.ff.fc2.\n",
      "\n",
      "Quantize weights module: trf_blocks.35.ff.fc3 ...\n",
      "[INFO] Done quantize linear layer weights - shape torch.Size([4096, 12288])\n",
      "Quantizate done : trf_blocks.35.ff.fc3.\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, CustomLinear):\n",
    "        print(f\"\\nQuantize weights module: {name} ...\")\n",
    "        module.quantize_weights()\n",
    "        print(f\"Quantizate done : {name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61093fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b345491-3510-4397-92d3-cd0a3fa3deee",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 4. Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b68ab489-48e5-471e-a814-56cda2d60f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "class Qwen3Tokenizer:\n",
    "    _SPECIALS = [\n",
    "        \"<|endoftext|>\",\n",
    "        \"<|im_start|>\", \"<|im_end|>\",\n",
    "        \"<|object_ref_start|>\", \"<|object_ref_end|>\",\n",
    "        \"<|box_start|>\", \"<|box_end|>\",\n",
    "        \"<|quad_start|>\", \"<|quad_end|>\",\n",
    "        \"<|vision_start|>\", \"<|vision_end|>\",\n",
    "        \"<|vision_pad|>\", \"<|image_pad|>\", \"<|video_pad|>\",\n",
    "        \"<think>\", \"</think>\"\n",
    "    ]\n",
    "    _SPLIT_RE = re.compile(r\"(<\\|[^>]+?\\|>|<think>|</think>)\")\n",
    "\n",
    "    def __init__(self, tokenizer_file_path=\"tokenizer.json\", repo_id=None,\n",
    "                 apply_chat_template=True, add_generation_prompt=False, add_thinking=False):\n",
    "\n",
    "        self.apply_chat_template = apply_chat_template\n",
    "        self.add_generation_prompt = add_generation_prompt\n",
    "        self.add_thinking = add_thinking\n",
    "\n",
    "        tok_file = Path(tokenizer_file_path)\n",
    "        self._tok = Tokenizer.from_file(str(tok_file))\n",
    "        self._special_to_id = {}\n",
    "        for t in self._SPECIALS:\n",
    "            tid = self._tok.token_to_id(t)\n",
    "            if tid is not None:\n",
    "                self._special_to_id[t] = tid\n",
    "\n",
    "        self.pad_token_id = self._special_to_id[\"<|endoftext|>\"]\n",
    "        self.eos_token_id = self.pad_token_id\n",
    "\n",
    "        if repo_id and \"Base\" not in repo_id:\n",
    "            eos_token = \"<|im_end|>\"\n",
    "        else:\n",
    "            eos_token = \"<|endoftext|>\"\n",
    "        if eos_token in self._special_to_id:\n",
    "            self.eos_token_id = self._special_to_id[eos_token]\n",
    "\n",
    "    def encode(self, text, chat_wrapped=None):\n",
    "        if chat_wrapped is None:\n",
    "            chat_wrapped = self.apply_chat_template\n",
    "\n",
    "        stripped = text.strip()\n",
    "        if stripped in self._special_to_id and \"\\n\" not in stripped:\n",
    "            return [self._special_to_id[stripped]]\n",
    "\n",
    "        if chat_wrapped:\n",
    "            text = self._wrap_chat(text)\n",
    "\n",
    "        ids = []\n",
    "        for part in filter(None, self._SPLIT_RE.split(text)):\n",
    "            if part in self._special_to_id:\n",
    "                ids.append(self._special_to_id[part])\n",
    "            else:\n",
    "                ids.extend(self._tok.encode(part).ids)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self._tok.decode(ids, skip_special_tokens=False)\n",
    "\n",
    "    def _wrap_chat(self, user_msg):\n",
    "        s = f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "        if self.add_generation_prompt:\n",
    "            s += \"<|im_start|>assistant\"\n",
    "            if self.add_thinking:\n",
    "                s += \"\\n\"\n",
    "            else:\n",
    "                s += \"\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b6df8bc-7308-468e-93ce-2d5529ea7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_REASONING_MODEL:\n",
    "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}/tokenizer.json\"\n",
    "else:\n",
    "    tokenizer_file_path = f\"Qwen3-{CHOOSE_MODEL}-Base/tokenizer.json\"\n",
    "\n",
    "tokenizer_file_path = os.path.join(MODEL_HUD_FOLDER, \"tokenizer.json\")\n",
    "\n",
    "hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    filename=\"tokenizer.json\",\n",
    "    local_dir=local_dir,\n",
    ")\n",
    "\n",
    "if USE_REASONING_MODEL or USE_INSTRUCT_MODEL:\n",
    "    tokenizer = Qwen3Tokenizer(\n",
    "        tokenizer_file_path=tokenizer_file_path,\n",
    "        repo_id=repo_id,\n",
    "        apply_chat_template=True,\n",
    "        add_generation_prompt=True,\n",
    "        # add_thinking=USE_REASONING_MODEL  # ignore\n",
    "        add_thinking=False\n",
    "    )\n",
    "else:\n",
    "    tokenizer = Qwen3Tokenizer(\n",
    "        tokenizer_file_path=tokenizer_file_path,\n",
    "        repo_id=repo_id,\n",
    "        apply_chat_template=False,\n",
    "        add_generation_prompt=False,\n",
    "        add_thinking=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1946b534-e3af-431a-a222-391a60bfa892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give me a short introduction to large language models.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language models.\"\n",
    "\n",
    "input_token_ids = tokenizer.encode(prompt)\n",
    "text = tokenizer.decode(input_token_ids)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d07df1-4401-4792-b549-7c4cc5632323",
   "metadata": {
    "id": "57d07df1-4401-4792-b549-7c4cc5632323"
   },
   "source": [
    "&nbsp;\n",
    "# 5. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5",
   "metadata": {
    "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5"
   },
   "outputs": [],
   "source": [
    "def generate_text_basic_stream(model, token_ids, max_new_tokens, eos_token_id=None):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = model(token_ids)[:, -1]\n",
    "            next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    "\n",
    "            if (eos_token_id is not None\n",
    "                   and torch.all(next_token == eos_token_id)):\n",
    "               break\n",
    "\n",
    "            yield next_token\n",
    "            \n",
    "            token_ids = torch.cat([token_ids, next_token], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d",
   "metadata": {
    "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d"
   },
   "outputs": [],
   "source": [
    "def get_clean_generated_text(generated_text):\n",
    "    output_text = \"\"\n",
    "    for token in generated_text:\n",
    "        token_id = token.squeeze(0).tolist()\n",
    "        text = tokenizer.decode(token_id)\n",
    "        output_text += text\n",
    "    # Post-processing to remove incomplete special tokens at the end\n",
    "    incomplete_special_token_pattern = re.compile(r\"<\\|[^>]*?$\")\n",
    "    output_text = re.sub(incomplete_special_token_pattern, \"\", output_text)\n",
    "    output_text = output_text.strip()\n",
    "    \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84410df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b3260bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response:\n",
      "The capital of Vietnam is Hanoi.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of VietNam?\"\n",
    "input_token_ids = tokenizer.encode(prompt)\n",
    "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
    "\n",
    "generated_text = generate_text_basic_stream(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=128,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "reponse = get_clean_generated_text(generated_text)\n",
    "print(\"Generated response:\")\n",
    "print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0d4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79421c2c",
   "metadata": {},
   "source": [
    "## 5.1. Measure time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0355abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GENERATED_TOKENS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36ba371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average time to generate 128 tokens: 4.06 seconds\n",
      "response: Ben Tre is a province located in the Mekong Delta region of Vietnam, known for its lush landscapes, rich agricultural heritage, and vibrant local culture. Heres a detailed description of the province:\n",
      "\n",
      "### Geography and Climate\n",
      "- **Location**: Ben Tre is situated in the southernmost part of Vietnam, bordering the provinces of Tin Giang to the north, Vnh Long to the east, and Bn Tre to the west.\n",
      "- **Climate**: The province experiences a tropical monsoon climate, characterized by high temperatures and humidity throughout the year. The rainy season typically runs from May to October, while the dry season lasts from November to\n"
     ]
    }
   ],
   "source": [
    "# Measure generation speed\n",
    "prompt = \"Describe the Ben Tre province in Vietnam in detail.\"\n",
    "input_token_ids = tokenizer.encode(prompt)\n",
    "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
    "\n",
    "# Warm up \n",
    "for i in range(2):\n",
    "    generated_text = generate_text_basic_stream(\n",
    "        model=model,\n",
    "        token_ids=input_token_ids_tensor,\n",
    "        max_new_tokens=MAX_GENERATED_TOKENS,\n",
    "        eos_token_id=tokenizer.eos_token_id)\n",
    "    reponse = get_clean_generated_text(generated_text)\n",
    "\n",
    "n_iter = 5\n",
    "start_time = time.time()\n",
    "for i in range(n_iter):\n",
    "    generated_text = generate_text_basic_stream(\n",
    "        model=model,\n",
    "        token_ids=input_token_ids_tensor,\n",
    "        max_new_tokens=MAX_GENERATED_TOKENS,\n",
    "        eos_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    reponse = get_clean_generated_text(generated_text)\n",
    "end_time = time.time()\n",
    "avg_time_per_iter = (end_time - start_time) / n_iter\n",
    "print(f\"\\nAverage time to generate {MAX_GENERATED_TOKENS} tokens: {avg_time_per_iter:.2f} seconds\")\n",
    "print(f\"response: {reponse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f529a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generated tokens: 128\n",
      "Throughput: 31.54 tokens/second\n"
     ]
    }
   ],
   "source": [
    "# number of generated tokens\n",
    "num_generated_tokens = len(tokenizer.encode(reponse))\n",
    "print(f\"Number of generated tokens: {num_generated_tokens}\")\n",
    "\n",
    "throughput = num_generated_tokens / avg_time_per_iter\n",
    "print(f\"Throughput: {throughput:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc1a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
